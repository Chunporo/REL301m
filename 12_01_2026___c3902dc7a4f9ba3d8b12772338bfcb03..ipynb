{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1b2a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma= 0.0  avg discounted return over 200 steps ≈ -1.150\n",
      "gamma= 0.5  avg discounted return over 200 steps ≈ -2.315\n",
      "gamma= 0.9  avg discounted return over 200 steps ≈ -14.416\n",
      "gamma=0.99  avg discounted return over 200 steps ≈ -162.549\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "STATES = [-2, -1, 0, 1, 2]  # temperature error\n",
    "A_OFF, A_ON = 0, 1\n",
    "\n",
    "def step(e, a):\n",
    "    \"\"\"\n",
    "    Toy dynamics:\n",
    "    - If ON: error tends to decrease by 1 (toward 0)\n",
    "    - If OFF: error tends to increase by 1 (away from 0)\n",
    "    plus small noise.\n",
    "    Động lực đồ chơi:\n",
    "    - Bật: sai lệch có xu hướng giảm 1 (tiến về 0)\n",
    "    - Tắt: sai lệch có xu hướng tăng 1 (xa 0 hơn)\n",
    "    cộng nhiễu nhỏ.\n",
    "    \"\"\"\n",
    "    noise = random.choice([-1, 0, 0, 0, 1])  # mostly 0\n",
    "    if a == A_ON:\n",
    "        e2 = e - 1 + noise\n",
    "    else:\n",
    "        e2 = e + 1 + noise\n",
    "    e2 = max(-2, min(2, e2))\n",
    "    r = -abs(e2)  # reward based on next state comfort\n",
    "    return e2, r\n",
    "\n",
    "def policy_threshold(e):\n",
    "    \"\"\"\n",
    "    Simple policy:\n",
    "    - If too warm/cold (|e|>=1), turn ON to correct\n",
    "    - If perfect (e==0), turn OFF\n",
    "    Chính sách đơn giản:\n",
    "    - Nếu lệch (|e|>=1) thì bật để kéo về 0\n",
    "    - Nếu e==0 thì tắt\n",
    "    \"\"\"\n",
    "    return A_OFF if e == 0 else A_ON\n",
    "\n",
    "def rollout_return(gamma, T=200, seed=0):\n",
    "    random.seed(seed)\n",
    "    e = random.choice(STATES)\n",
    "    G = 0.0\n",
    "    pow_g = 1.0\n",
    "    for _ in range(T):\n",
    "        a = policy_threshold(e)\n",
    "        e, r = step(e, a)\n",
    "        G += pow_g * r\n",
    "        pow_g *= gamma\n",
    "    return G\n",
    "\n",
    "gammas = [0.0, 0.5, 0.9, 0.99]\n",
    "for g in gammas:\n",
    "    avg = sum(rollout_return(g, T=200, seed=s) for s in range(20)) / 20\n",
    "    print(f\"gamma={g:>4}  avg discounted return over 200 steps ≈ {avg:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4f86c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = 40.0\n",
      "Episode 2: Total Reward = 21.0\n",
      "Episode 3: Total Reward = 16.0\n",
      "Episode 4: Total Reward = 14.0\n",
      "Episode 5: Total Reward = 12.0\n",
      "Episode 6: Total Reward = 13.0\n",
      "Episode 7: Total Reward = 19.0\n",
      "Episode 8: Total Reward = 17.0\n",
      "Episode 9: Total Reward = 17.0\n",
      "Episode 10: Total Reward = 23.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym  # Import Gymnasium\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 2. Initialize the agent's parameters\n",
    "state, _ = env.reset()  # The reset method now returns a tuple (state, info)\n",
    "done = False  # Indicator for episode completion\n",
    "total_reward = 0  # Variable to track total reward during an episode\n",
    "\n",
    "# 3. Define Q-learning parameters (for simplicity, we use a basic agent here)\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 10  # Total episodes to run\n",
    "\n",
    "# Initialize Q-table (simple state-action space)\n",
    "# CartPole has 4 continuous state variables, so here we'll discretize it into a smaller grid\n",
    "state_space_bins = [6, 12, 12, 6]  # Discretizing each dimension into bins\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(state_space_bins[0], state_space_bins[1], state_space_bins[2], state_space_bins[3], env.action_space.n))\n",
    "\n",
    "# Function to discretize continuous state space\n",
    "def discretize_state(state):\n",
    "    \"\"\"Discretizes the continuous state to a discrete index in the Q-table\"\"\"\n",
    "    state_bins = []\n",
    "    for i in range(len(state)):\n",
    "        # Clip the value of each dimension within the allowed bin range\n",
    "        bin_idx = np.digitize(state[i], np.linspace(-1, 1, state_space_bins[i]))\n",
    "        # Ensure that the index is within the bounds of the state space bins\n",
    "        state_bins.append(min(bin_idx, state_space_bins[i] - 1))  # Clip to avoid out-of-bounds indexing\n",
    "    return tuple(state_bins)\n",
    "\n",
    "# 4. Training loop (run episodes)\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # Reset the environment and extract the state\n",
    "    state = discretize_state(state)  # Discretize the continuous state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # 5. Agent selects an action using epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit: choose the best action based on Q-table\n",
    "        \n",
    "        # 6. Take the action and observe the result\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "        \n",
    "        # 7. Update Q-value based on the agent's experience\n",
    "        q_table[state + (action,)] = q_table[state + (action,)] + learning_rate * (reward + discount_factor * np.max(q_table[next_state]) - q_table[state + (action,)])\n",
    "        \n",
    "        # Update the state and total reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c9d89bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    feature1  feature2  feature3\n",
      "0          1         5         5\n",
      "1          2         4         4\n",
      "2          3         3         3\n",
      "3          4         2         2\n",
      "4          5         1         1\n",
      "5          1         5         5\n",
      "6          2         4         4\n",
      "7          3         3         3\n",
      "8          4         2         2\n",
      "9          5         1         1\n",
      "10         7         3         3\n",
      "11         8         4         4\n",
      "12         9         5         5\n",
      "13        10         6         6\n",
      "14        11         7         7\n",
      "Episode 1: Total Reward = 27.0\n",
      "Episode 2: Total Reward = 11.0\n",
      "Episode 3: Total Reward = 38.0\n",
      "Episode 4: Total Reward = 35.0\n",
      "Episode 5: Total Reward = 18.0\n",
      "Episode 6: Total Reward = 11.0\n",
      "Episode 7: Total Reward = 13.0\n",
      "Episode 8: Total Reward = 15.0\n",
      "Episode 9: Total Reward = 23.0\n",
      "Episode 10: Total Reward = 11.0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym  # Import Gymnasium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Kaggle (assuming CSV format)\n",
    "df = pd.read_csv('path_to_your_kaggle_dataset.csv')  # Replace with the actual dataset path\n",
    "\n",
    "# Print the first few rows to check the structure\n",
    "print(df.head(15))\n",
    "\n",
    "# Extract relevant features from the dataset\n",
    "# Assume the dataset has columns like 'feature1', 'feature2', etc., that we can use\n",
    "features = df[['feature1', 'feature2', 'feature3']].values  # Example features\n",
    "\n",
    "# 1. Create the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 2. Initialize the agent's parameters\n",
    "state, _ = env.reset()  # The reset method now returns a tuple (state, info)\n",
    "done = False  # Indicator for episode completion\n",
    "total_reward = 0  # Variable to track total reward during an episode\n",
    "\n",
    "# 3. Define Q-learning parameters (for simplicity, we use a basic agent here)\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 10  # Total episodes to run\n",
    "\n",
    "# Initialize Q-table (simple state-action space)\n",
    "# CartPole has 4 continuous state variables, so here we'll discretize it into a smaller grid\n",
    "state_space_bins = [6, 12, 12, 6]  # Discretizing each dimension into bins\n",
    "q_table = np.random.uniform(low=-1, high=1, size=(state_space_bins[0], state_space_bins[1], state_space_bins[2], state_space_bins[3], env.action_space.n))\n",
    "\n",
    "# Function to discretize continuous state space\n",
    "def discretize_state(state):\n",
    "    \"\"\"Discretizes the continuous state to a discrete index in the Q-table\"\"\"\n",
    "    state_bins = []\n",
    "    for i in range(len(state)):\n",
    "        # Clip the value of each dimension within the allowed bin range\n",
    "        bin_idx = np.digitize(state[i], np.linspace(-1, 1, state_space_bins[i]))\n",
    "        # Ensure that the index is within the bounds of the state space bins\n",
    "        state_bins.append(min(bin_idx, state_space_bins[i] - 1))  # Clip to avoid out-of-bounds indexing\n",
    "    return tuple(state_bins)\n",
    "\n",
    "# 4. Training loop (run episodes)\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # Reset the environment and extract the state\n",
    "    state = discretize_state(state)  # Discretize the continuous state\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # 5. Agent selects an action using epsilon-greedy strategy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Explore: random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit: choose the best action based on Q-table\n",
    "        \n",
    "        # 6. Take the action and observe the result\n",
    "        next_state, reward, done, _, info = env.step(action)\n",
    "        next_state = discretize_state(next_state)\n",
    "        \n",
    "        # 7. Update Q-value based on the agent's experience\n",
    "        q_table[state + (action,)] = q_table[state + (action,)] + learning_rate * (reward + discount_factor * np.max(q_table[next_state]) - q_table[state + (action,)])\n",
    "        \n",
    "        # Update the state and total reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
