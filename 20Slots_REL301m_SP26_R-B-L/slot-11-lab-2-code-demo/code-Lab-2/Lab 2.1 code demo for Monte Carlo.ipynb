{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T3EIYS-HLhk"
      },
      "outputs": [],
      "source": [
        "# Lab code demo for Monte Carlo- HoaDNT@fe.edu.vn\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = (3, 4)\n",
        "        self.num_actions = 4  # Up, Down, Left, Right\n",
        "        self.rewards = np.array([\n",
        "            [0, 0, 0, 0],\n",
        "            [0, 0, 0, 0],\n",
        "            [0, 0, 0, 1]  # Reward of +1 in the bottom-right cell\n",
        "        ])\n",
        "        self.start_state = (2, 0)\n",
        "\n",
        "    def step(self, state, action):\n",
        "        # Define the dynamics of the environment\n",
        "        row, col = state\n",
        "        if action == 0:  # Up\n",
        "            row = max(0, row - 1)\n",
        "        elif action == 1:  # Down\n",
        "            row = min(self.grid_size[0] - 1, row + 1)\n",
        "        elif action == 2:  # Left\n",
        "            col = max(0, col - 1)\n",
        "        elif action == 3:  # Right\n",
        "            col = min(self.grid_size[1] - 1, col + 1)\n",
        "        next_state = (row, col)\n",
        "        reward = self.rewards[row, col]\n",
        "        return next_state, reward\n",
        "\n",
        "def monte_carlo(grid_world, num_episodes, gamma=1.0):\n",
        "    returns_sum = np.zeros(grid_world.grid_size)\n",
        "    returns_count = np.zeros(grid_world.grid_size)\n",
        "    V = np.zeros(grid_world.grid_size)\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(grid_world)\n",
        "        visited_states = set()\n",
        "        for t, (state, action, reward) in enumerate(episode):\n",
        "            if state not in visited_states:\n",
        "                visited_states.add(state)\n",
        "                G = sum([gamma**i * step[2] for i, step in enumerate(episode[t:])])\n",
        "                returns_sum[state] += G\n",
        "                returns_count[state] += 1\n",
        "                V[state] = returns_sum[state] / returns_count[state]\n",
        "\n",
        "    return V\n",
        "\n",
        "def generate_episode(grid_world):\n",
        "    episode = []\n",
        "    state = grid_world.start_state\n",
        "    while True:\n",
        "        action = np.random.choice(grid_world.num_actions)\n",
        "        next_state, reward = grid_world.step(state, action)\n",
        "        episode.append((state, action, reward))\n",
        "        if next_state == (2, 3):  # Terminal state\n",
        "            break\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# Create a grid world environment\n",
        "grid_world = GridWorld()\n",
        "\n",
        "# Run Monte Carlo to estimate the state-value function\n",
        "num_episodes = 1000\n",
        "V = monte_carlo(grid_world, num_episodes)\n",
        "\n",
        "# Print the estimated state-value function\n",
        "print(\"Estimated State-Value Function:\")\n",
        "print(V)"
      ]
    }
  ]
}